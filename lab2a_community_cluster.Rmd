---
title: "EDS232 Lab2a: Community Cluster"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Learning Objectives

### In this lab, you will play with unsupervised classification techniques while working with ecological community datasets.

- Comparing species counts between sites using distance metrics:
- Euclidean calculates the distance between a virtualized space using Pythagorean theorem.
- Manhattan calculates integer “around the block” difference.
- Bray-Curtis dissimilarity is based on the sum of lowest counts of shared species between sites over the sum of all species. A dissimilarity value of 1 is completely dissimilar, i.e. no species shared. A value of 0 is completely identical.

### Clustering

K-Means clustering with function kmeans() given a pre-assigned number of clusters assigns membership centroid based on reducing within cluster variation.
- Voronoi diagrams visualizes regions to nearest points, useful here to show membership of nodes to nearest centroid.
Hierarchical clustering allows for a non-specific number of clusters.
- Agglomerative hierarchical clustering, such as with diana(), agglomerates as it builds the tree. It is good at identifying small clusters.
- Divisive hierarchical clustering, such as with agnes(), divides as it builds the tree. It is good at identifying large clusters.
- Dendrograms visualize the branching tree.
Ordination (coming Monday)

## 1 Clustering

Clustering associates similar data points with each other, adding a grouping label. It is a form of unsupervised learning since we don’t fit the model based on feeding it a labeled response (i.e. y).

### 1.1 K-Means Clustering
Source: [K Means Clustering in R | DataScience+](https://datascienceplus.com/k-means-clustering-in-r/)

In k-means clustering, the number of clusters needs to be specified. The algorithm randomly assigns each observation to a cluster, and finds the centroid of each cluster. Then, the algorithm iterates through two steps:

1. Reassign data points to the cluster whose centroid is closest.
1. Calculate new centroid of each cluster.

These two steps are repeated until the within cluster variation cannot be reduced any further. The within cluster variation is calculated as the sum of the euclidean distance between the data points and their respective cluster centroids.

#### 1.1.1 Load and plot the iris dataset

```{r}
# load R packages
librarian::shelf(
  dplyr, DT, ggplot2, skimr, tibble)

# set seed for reproducible results
set.seed(42)

# load the dataset
data("iris")

# look at documentation in RStudio
if (interactive())
  help(iris)

# show data table
datatable(iris)
```

```{r}
# skim the table for a summary
skim(iris)
```

 
```{r}
# remove the rows with NAs
penguins <- na.omit(iris)

# plot petal length vs width, species naive
ggplot(
  iris, aes(Petal.Length, Petal.Width)) +
  geom_point()
```

```{r}
# plot petal length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  iris, aes(Petal.Length, Petal.Width, color = Species)) +
  geom_point() +
  legend_pos
```

#### 1.1.2 Cluster iris using kmeans()

```{r}
# cluster using kmeans
k <- 3  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# show cluster result
iris_k
```

```{r}
# compare clusters with species (which were not used to cluster)
table(iris_k$cluster, iris$Species)
```
Question: How many observations could be considered “misclassified” if expecting petal length and width to differentiate between species?

**6 observations could be considered "misclassified" if we expect petal length and width to differentiate between species.**

```{r}
# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos
```

#### 1.1.3 Plot Voronoi diagram of clustered iris

This form of clustering assigns points to the cluster based on nearest centroid. You can see the breaks more clearly with a [Voronoi diagram.](https://en.wikipedia.org/wiki/Voronoi_diagram)

```{r}
librarian::shelf(ggvoronoi, scales)

# this tribble part differs from the penguins code:
# define bounding box for geom_voronoi()
box <- tribble(
  ~Petal.Length, ~Petal.Width, ~group,
  1, 0.1, 1,
  1, 2.5, 1,
  7, 2.5, 1,
  7, 0.1, 1,
  1, 0.1, 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

Task: Show the Voronoi diagram for fewer (k=2) and more (k=8) clusters to see how assignment to cluster centroids work.

```{r}
# cluster using kmeans with k = 2
k <- 2  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

```{r}
# cluster using kmeans with k = 8
k <- 8  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

### 1.2 Hierarchical Clustering

Next, you’ll cluster sites according to species composition. You’ll use the `dune` dataset from the `vegan` R package.

#### 1.2.1 Load dune dataset

```{r}
librarian::shelf(
  cluster, vegan)

# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```

Question: What are the rows and columns composed of in the dune data frame?

**In the dune dataframe, the rows are composed of sites (n=20) and the columns are plant species (n=30).**

#### 1.2.2 Calculate Ecological Distances on sites

Before we calculate ecological distance between sites for dune, let’s look at these metrics with a simpler dataset, like the example given in Chapter 8 by Kindt and Coe (2005).

```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites
```

```{r}
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan
```

Sites B & C have the largest manhattam distance between them. This aligns with the results of the euclidean distance, which shows the greatest distance between sites B & C. A & C have the smallest manhattan distance between them, which makes sense because 1 is close to 0, but the values of 5 for the B site is further from the 0 values at the C site. 

```{r}
sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean
```

Sites B & C have the largest euclidean distance between them. Sites A and C have the smallest euclidean distance between them. This makes sense because the values of 5 for the B site result in a larger quanititative difference from 0 than the values of 1 for the A site. 

```{r}
sites_bray <- vegdist(sites, method="bray")
sites_bray
```

1 = completely dissimilar
0 = indentical

This output shows that sites A & B are mostly dissimilar, as they align perfectly with species 3, but are very dissimilar for species 1 & 2. Sites A & C are perfectly dissimilar as they are different (1 or 0) for every species. B & C are completely dissimilar as well, as they are different (5 or 0) for every species. This aligns with the euclidian results that showed that B & C have the largest distance between them. 

#### 1.2.3 Bray-Curtis Dissimilarity on sites

Let’s take a closer look at the [Bray-Curtis Dissimilarity](https://en.wikipedia.org/wiki/Bray%E2%80%93Curtis_dissimilarity) distance:

![](images/bray_curtis_eqns.png)

#### 1.2.4 Agglomerative hierarchical clustering on dune

See text to accompany code: HOMLR [21.3.1 Agglomerative hierarchical clustering.](https://bradleyboehmke.github.io/HOML/hierarchical.html#agglomerative-hierarchical-clustering)

```{r}
# Dissimilarity matrix
d <- vegdist(dune, method="bray")
dim(d)
```

```{r}
as.matrix(d)[1:5, 1:5]
```
```{r}
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)
```

```{r}
# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac
```

```{r}
# Dendrogram plot of hc2
plot(hc2, which.plot = 2)
```

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

```{r}
# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac
```

```{r}
# Dendrogram plot of hc3
plot(hc3, which.plot = 2)
```

#### 1.2.5 Divisive hierarchical clustering on dune

See text to accompany code: [HOMLR 21.3.2 Divisive hierarchical clustering.](https://bradleyboehmke.github.io/HOML/hierarchical.html#divisive-hierarchical-clustering)

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc
```

The Divisive clustering approach using `diana()` produced a smaller coefficient that represents clustering structure found (`r hc4$dc`), compared to the coefficient produced by the agglomerative approach using `agnes()` (`r hc3$ac`). This aligns with my understanding of these 2 approaches, because the agglomerative approach is better at identifying smaller groups, starting at the bottom of the dendrogram and moving up, so the clustering coefficient is larger. 

#### 1.2.6 Determining optimal clusters

See text to accompany code: [HOMLR 21.4 Determining optimal clusters.](https://bradleyboehmke.github.io/HOML/hierarchical.html#determining-optimal-clusters)

```{r}
librarian::shelf(factoextra)

# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

#### 1.2.7 Working with dendrograms

See text to accompany code: [HOMLR 21.5 Working with dendrograms.](https://bradleyboehmke.github.io/HOML/hierarchical.html#working-with-dendrograms)

```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])
```

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)
```

```{r}
# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```





























